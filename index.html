<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Qiuyuan Huang</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="topnav">
      <div class="topnav-left">
        <img src="Q5.png"  width="115" height="115" alt="Microsoft Research">
         <a href="index.html">Qiuyuan (Enno) Huang</a>      
      </div>            
    </div>    <div class="header">
      <p></p>
    </div>
    <div class="sidenav">
      <a href="#Research Interest">Research Interest </a>
      <a href="#Selected Works">Selected Works</a>
      <a href="#Publications">Publications</a>
      <a href="#ToolDownload">Data Collection Tools</a>
      <a href="#EULA">User License Agreement</a>
      <a href="#Fun">Fun</a>
    </div>
    <div class="main">
      <p>I am a Principal Researcher at Microsoft Research (MSR), Redmond, WA. 
        My current research interests are in deep learning in general, and embodied multi-modal trustworthy complex intelligence in particular. 
        More specifically, my primary researches particularly focused on building generalist embodied multi-modal foundations towards AGI. 
        
      </p>
      
      <h3 id="Research Interest">Research Interest </h3>
      <h5 <b> <span style="color:#9dbad1;">
        <div> PS:<b><span style="color:#9c0a1c;"> Confidential Projects </b> are not listed on the website, which include but not limited to: </div>
        <div>• MSR-OpenAI Confidential Project [embodied action-multimodality pretraining, finetune, post-training, in-contextual learning] ; </div>
        <div>• Trustworthy Autonomous System [RL/IL/MPC/Decision-making] for Human-in-the-loop with Human Value Alignment; </div>
        <div>• Data-driven Neuralcomputation-HCI in Dynamic Spatial Environment [humaniod robotics, bio-wearable machine, aerospace navigation, etc.] </div>
        </b> 
      </h5>
        <p>
          Broadly, my work focuses on embodied AI with lagre multi-modal foundation models generation, and how to inform machines about the world which interact with humans. 
          The research areas include infinite embodied foundation model generation for AI agents (pre-training, in-contextual learning, post-training with RL and IL), 
          and neural-symbolic interpretable representation for knowledge inference reasoning with vision-language agent acquisition and prediction. 
          The recent topics of work include:
          <p>
          <b> <span style="color:#9dbad1;"><b> ♢ Foundation Models: Generalist Embodied Foundation Model with Trustworthy Multimodal Agent Intelligence </b></b>
            <div> • Embodied Foundation Model (pertraining and finetune) for 
              <div> i) robotics [manupulation, navigation, gesture, grasp, locomotion, teleoperation, execution];</div> 
              <div> ii) general vision-language tasks [video-audio-language, visual-captioning, VQA, generactive AI, classification tasks, etc.]; </div>
              <div> iii) 2D->3D/Simulation/VR/AR/Gaming/Mix-reality [simulation and real world models and agents]; </div>
              <div> iv) embodied-healthcare. </div>
              </div>
            <div> • Trustworthy Interactive AI Agents System Generation and Multi-agent Infrastructure Optimization in Sim2Real (simulation models to real robotics/VR/gaming/3D scene transferring); </div>
            <div> • Large Vision-Language Per-training on Multi-modality for Generative AI; <div> 
            <div> • Embodied AI for Mimicking Human Behavior with Empathy Alignment. <div> 
          </p>
          <p>
          <b> <span style="color:#9dbad1;"><b>♢ Algorithms: Embodied AI with Reinforcement Learning (RL), Imitation Learning (IL) 
            , Model Predictive Control (MPC), and Decision-Making </b></b>
            <div> • Post-Training for Generative AI with RL/IL/MPC;</div>
            <div> • Embodied AI for Vision-Language with RL/GAN;</div>
            <div> • Multi-Agent Embodied System with Optimization Inference in Model Predictive Control (MPC) and In-contextual Learning. </div> 
          </p>
          <p>
          <b> <span style="color:#9dbad1;"><b>♢ Inference System: Trustworthy Interactive Intelligence with Cognitive Complexes Interpretability  </b></b>
            <div> • Knowledge Inference Reasoning Representation for Large Foundation Models in Cognition Complex with Trustworthy Dataverse
              (mimic human behavior and inference-interpretability); </div>
            <div> • Tensor Product Representation of Inference Neural Symbolic Foundation Model with Interpretability. </div>
          </p>
          and 
          During the past year, my research at MSR has centered around the theme of Infinite Embodied AI 
         for pre-trained large multimodal foundation models —an autonomous system that integrates large foundation models into interactive agnet 
        with trustworthy value alignment.
         
        </p>

        <p>          
          I believe, by integrating fine-grained structured information, we can achieve better yet interpretable, grounded and robust multi-modality intelligent agents for cross-modality and agnostic-reality.
          Beyond pushing state-of-the-art research, I am a strong proponent of efficient and reproducible research. We also helped to ship AI techniques to Microsoft products and to create real-world impact.
          I enjoy the process of bridging the gap between theory and practice, bringing theoretical results to practical applications. 
          I also enjoy coding, building real systems from scratch, and making them work simply, efficiently, and elegantly. 
        </p>
      
      <h3 id="Selected Works">Selected Works</h3>
        <h4 id="Task1">My research is targeted at research & practical applications involving:</h4>
          <p>
          <b>• Embodied Action Foundation Model:</b> Large embodied foundation models for flexible AI agent infrastructure with vision-language. 
          "An Interactive Agent Foundation Model" of large action-vision-language per-training for embodied AI in Robotics, Gaming/mix-reality, 
          and Embodied Healthcare. The model developed in collaboration with MS Product Robotics team, Gaming Team, Turing team, and Stanford.
         
          <a href="https://agentfoundationmodel.github.io" class="button">Demo Link</a>
           
          </p>

         <p>
          <b>• Multi-agent Infrastructure with In-contextual Learning:</b> "MindAgent: multi-agent infrastructure with GPT-4 (V) ison 
          for infinite spatial intelligence in real and virtual world", in collaboration with MS X-box team, and shipped to MS Gaming team; 
           <a href="https://mindagent.github.io" class="button">Demo Link</a>
           
          <p style="text-align: justify; font-size: 1.0em;">We present <tt>MindAgent</tt>, an infrastructure for emergent gaming interaction, 
           enables<span style="color:#f1f0f1;"> <b>multi agents collaboration</b></span> and <span style="color:#f1f0f1;">
           <b>human-agent collaboration</b></span>.
          </p>      
          </p>

        <p>
          <b>• Post-Training for Generative AI:</b> dynamic interaction model generation (GenAI) in multimodality via emergent abilities. "ArK: 
          Augmented reality with emergent infrastructure". Post-training using reinforcement and imitation learning for generative AI (GPT, Dall-E), 
          in collaboration with MS Mesh team; the model has been shipped to MS office teams;
                 
        <a href="https://arxiv.org/pdf/2305.00970" class="button">Link</a>
               
        </p>
      
        <p>
          <b>• Applications for Human-Machine Interaction:</b> Embodied Multimodal Navigation with RL & IL. "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning 
          for Vision-Language Navigation" for simulation agent robotics, in collaboration with MS Cognition Services team;
                  
        <a href="https://arxiv.org/pdf/1811.10092" class="button">Link</a>
         <a href="https://www.microsoft.com/en-us/research/video/reinforced-cross-modal-matching-and-self-supervised-imitation-learning-for-vision-language-navigation/" class="button">Demo Link</a>                
        </p>  

         <p>
          <b>• Vision-Language Pre-training:</b> "Training Vision-Language Transformers from Captions", 
           Vision-language pre-training model generation in collaboration with Turing team and Cognition Services;
                   
         <a href="https://arxiv.org/abs/2205.09256" class="button">Link</a>
                 
         </p> 

        <p>
          <b>• Knowledge Inference and Representation in Multi-modality:</b> Knowledge-Inference Reasoning Transformer: 
          <div>          
          i) "KAT: A Knowledge Augmented Transformer for Vision-and-Language", <a href="https://arxiv.org/abs/2112.08614" class="button">Link</a>
          </div> 
          <div>          
          ii) "Retrieve What You Need: knowledge-LLM agent with GPT", <a href="https://aclanthology.org/2024.tacl-1.14/" class="button">Link</a>
          </div>
          <div>          
          iii) "Logical Transformer", which collaborated with MS Turing Team and Bing search. 
          <a href="https://aclanthology.org/2023.findings-acl.111.pdf" class="button">Link</a>
          </div>        
        </p> 
              
        
        <h3 id="Publications">Selected Publications </h3>          
        <h4 id="Task1">(please refer to <a href="https://scholar.google.com/citations?user=U7Mmyc8AAAAJ&hl=en" class="button">Google Scholar</a> for the full publications)</h4>          
           
        <p>
          <b>■ An Interactive Agent Foundation Model</b> for embodied interaction in Robot, Gaming, and Healthcare.
        <div>
          Z. Durante*, R. Gong*, R. Taori, Y. Noda, P. Tang, 
          E. Adeli, S. Kowshika Lakshmikanth, K. Schulman, A. Milstein, D. Terzopoulos, A. Famoti, 
          N. Kuno, A. Llorens, H. Vo, K. Ikeuchi, L. Fei-Fei, J. Gao, N. Wake*▶, Q. Huang*▶.    ∗Equal Contribution. ▶Project Lead.
        </div>          
          arXiv:2402.05929, May 2024.
          <a href="https://arxiv.org/abs/2402.05929">[paper]</a>
          <a href="https://agentfoundationmodel.github.io">[webpage]</a>           
         </p>

        <p>
          <b>■ Agent AI Towards a Holistic Intelligence.</b>
        <div>
          Q. Huang, N. Wake, Z. Durante, R. Gong, R. Taori, Y. Noda, D. Terzopoulos, 
          N. Kuno, A. Famoti, A. Llorens, J. Langford, H. Vo, L. Fei-Fei, K. Ikeuchi, J. Gao.        
        </div>          
          arXiv:2403.00833, May 2024.          
          <a href="https://arxiv.org/abs/2403.00833">[paper]</a>        
    </p>

        <p> 
          <b>■ MindAgent: Emergent Gaming Interaction.</b>
        <div>R. Gong*, Q. Huang*▶, X. Ma*, H. Vo, Z. Durante, Y. Noda, Z. Zheng, 
          S. Zhu, D. Terzopoulos, L. Fei-Fei, J. Gao.  
          *Equal Contribution. ▶Project Lead.        
        </div>          
        Proceedings of NAACL 2024, Jun. 2024.          
          <a href="https://arxiv.org/abs/2309.09971">[paper]</a>
          <a href="https://mindagent.github.io">[webpage]</a>         
    </p>  

    <p> 
          <b>■ Agent AI: Surveying the Horizons of Multimodal Interaction.</b>
        <div>Z. Durante*, Q. Huang*▶, N. Wake*, R. Gong, J. Park, B. Sarkar, 
          R. Taori, Y. Noda, D. Terzopoulos, Y. Choi, K. Ikeuchi, H. Vo, L. Fei-Fei, J. Gao.   *Equal Contribution. ▶Project Lead.</div>          
        arXiv:2401.03568, Jan 2024.
          <a href="https://arxiv.org/abs/2401.03568">[paper]</a>         
    </p> 

  <p> 
          <b>■ Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering.</b>
        <div>D. Wang, Q. Huang, M. Jackson, J. Gao</div>          
        Proceedings of Transactions of ACL (TACL) 2024. Apr. 2024
      <a href="https://aclanthology.org/2024.tacl-1.14/">[paper]</a>         
    </p>       

    <p> 
          <b>■ ArK: Augmented Reality with Knowledge Interactive Emergent Ability. (Generative AI)   </b>
        <div>Q. Huang, J. Park, A. Gupta, P. Bennett, R. Gong, S. Som, B. Peng, O. Mohammed, C. Pal, Y. Choi, J. Gao.         
        </div>          
        arXiv:2305.00970. Shipped the model in Microsoft Teams for product, collobarated with Microsoft with Mesh team. Jun. 2023
      <a href="https://arxiv.org/abs/2305.00970">[paper]</a> 
      <a href="https://augmented-reality-knowledge.github.io">[webpage]</a>
      <a href="https://www.microsoft.com/en-us/research/project/mixed-reality/">[page]</a> 
    </p>

      

    <p> 
          <b>■ Localized Symbolic Knowledge Distillation: Infinite Knowledge Distillation in Multi-modality (Knowledge-auto-GPT)</b>
        <div>J. Park, J. Hessel, K. Chandu, P. Liang, X. Lu, P. West, Y. Yu, Q. Huang, 
          J. Gao, A. Farhadi, Y. Choi
        </div>          
        Proceedings of NeurIPS 2023. Dec.2023
          <a href="https://arxiv.org/abs/2312.04837">[paper]</a>        
    </p>  

        <p> 
          <b>■ Logical Transformers: Infusing Logical Structures into Pre-Trained Language Models</b> 
        <div>B. Wang, Q. Huang, B. Deb, A. Halfaker, L. Shao, D. McDuff, A. Awadallah, D. Radev, J. Gao</div>          
          Proceedings of ACL 2023. Jul.2023
            <a href="https://aclanthology.org/2023.findings-acl.111.pdf">[paper]</a>   
      </p> 

        <p> 
          <b>■ Training Vision-Language Transformers from Captions (Vision-Lanaguage Pertraining)</b> 
        <div>L. Gui, Y. Chang, Q. Huang, S. Som, A. Hauptmann, J. Gao, Y. Bisk.</div>          
          Proceedings of Transactions on Machine Learning Research. 2023
      <a href="https://arxiv.org/abs/2205.09256">[paper]</a>   
      </p> 

      
      
    <p> 
          <b>■ KAT: A Knowledge Augmented Transformer for Vision-and-Language.</b> <span style="color:#9c0a1c;"><b> SoTA of OK-VQA Leaderboard (2021).</b>
        <div>L. Gui, B. Wang, Q. Huang, A. Hauptmann, Y. Bisk, J. Gao
        </div>          
        Proceedings of NAACL 2022, Jun. 2022.  
          <a href="https://arxiv.org/abs/2112.08614">[paper]</a>   
      </p> 


   <p> 
          <b>■ Vision-Language Navigation Policy Learning and Adaptation.</b> 
        <div>X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y. Wang, W. Wang, L. Zhang
        </div> 
        IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). Jun. 2020. <a href="https://ieeexplore.ieee.org/document/8986691">[paper]</a>    
       </p>      

    
       <p> 
          <b>■ Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation.</b> 
            <span style="color:#9c0a1c;"><b> Best Student Paper Award.</b>
        <div>X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y. Wang, W. Wang, L. Zhang
        </div> 
        Proceedings of CVPR 2019. Jun. 2019. <a href="https://arxiv.org/abs/1811.10092">[paper]</a>    
       </p>  

      
      <p> 
          <b>■ Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations.</b>
        <div> K. Chen, Q. Huang, H. Palangi, P. Smolensky, K. Forbus, J. Gao
        </div>          
         Proceedings of ICML 2020. Feb.2020          
      <a href="https://arxiv.org/abs/1910.02339">[paper]</a>   
      </p>  

      <p> 
          <b>■ TP-N2F: Tensor Product Representation for Natural To Formal Language Generation.</b> <span style="color:#9c0a1c;"><b>Best Paper Award.</b>
        <div>K. Chen, Q. Huang, H. Palangi, P. Smolensky, K. Forbus, J. Gao. </div>          
          Proceedings of NeurIPS workshop 2019. Dec.2019.  
          <a href="https://arxiv.org/abs/1910.02339v2">[paper]</a>   
      </p>  


       <p> 
          <b>■ Attentive Tensor Product Learning.</b> 
        <div>Q. Huang, L. Deng, O. Wu, C. Liu, X.He</div>          
          Proceedings of AAAI 2019. Feb. 2019
          <a href="https://arxiv.org/abs/1802.07089">[paper]</a>   
      </p> 

       <p> 
          <b>■ Turbo Learning for CaptionBot and DrawingBot.</b> 
        <div>Q. Huang, P. Zhang, D. Wu, L. Zhang.</div>          
          Proceedings of NeurIPS 2018. Dec.2018.  
          <a href="https://arxiv.org/abs/1805.08170">[paper]</a>   
      </p>  

        <p> 
          <b>■ Hierarchically Structured Reinforcement Learning for Topically Coherent Visual (video) Story Generation.</b> 
        <div> Q. Huang*, Z. Gan*, A. Celikyilmaz, L. Li, D. Wu, X. He. *Equal contribution. </div>          
          Proceedings of AAAI 2019. Feb.2019          
       <a href="https://arxiv.org/abs/1805.08191">[paper]</a>   
      </p> 

       <p> 
          <b>■ Tensor Product Generation Networks for Deep NLP Modeling.</b> 
        <div>Q. Huang, P. Smolensky, X. He, L. Deng, D. Wu, Long paper.</div>          
          Proceedings of NAACL 2018, ACL–Association for Computational Linguistics, New Orleans, Louisiana, US. Jun.1- Jun.6. 2018.
     <a href="https://aclanthology.org/N18-1114/">[paper]</a>   
      </p> 

         <p> 
          <b>■ AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks.</b> 
        <div>T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, X. He.</div>          
          Proceedings of CVPR 2018. Jun.2018          
      <a href="https://arxiv.org/abs/1711.10485">[paper]</a>   
      </p> 

          <p> 
          <b>■ Martian--message broadcast via LED lights to heterogeneous smartphones: poster.</b> <span style="color:#9c0a1c;"><b>Best Poster Paper Award.</b>        
            <div> H. Du, J. Han, Q. Huang, X. Jian, C. Bo, Y. Wang, X.-Y. Li.</div>          
          Proceedings of the 22nd ACM MobiCom, 417-418. Feb. 2016
         <a href="https://dl.acm.org/doi/abs/10.1145/2973750.2985257">[paper]</a>   
          </p> 

      


      
      
      
      <h3 id="ToolDownload">Data Collection Tools Download</h3>
        <p>
          <b>Data Collection: </b> For our released <a href="https://nicedataset.github.io">“NICE” dataset and banchmark</a>, <a href="https://github.com/mindagent/mindagent">“Cuisine world” dataset, benchmark, and infrastructure</a> etc. You can find the codes for data collection below: 
        </p>
        <p><a href="https://github.com/NICEdataset/DataCollectionTool" class="button">Data Collection Tools</a></p>
       
      
      <h3 id="EULA">User License Agreement (EULA) for Using Our Dataset</h3>
        <p>
          You should complete an end user license agreement (EULA) before accessing the dataset/benchmark/infractrcuture. 
          The EULA will be posted soon once the data releasing process is finished. 
        </p>
        <a href="https://github.com/nicedataset/NICEdataset.github.io/blob/master/End%20User%20License%20Agreement.pdf" class="button">Download EULA</a>
      
      
      <h3 id="Fun">Fun</h3>
        <p>
        <p>Ballet, Violin, Tennis, Hiking</p>
        </p>
        
        
    </div>
  </body>

</html>
