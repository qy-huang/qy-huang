<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Qiuyuan Huang</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="topnav">
      <div class="topnav-left">
        <img src="qihua_r.jpg"  width="95" height="95" alt="Microsoft Research">
         <a href="index.html">Qiuyuan Huang</a>      
      </div>            
    </div>    <div class="header">
      <p></p>
    </div>
    <div class="sidenav">
      <a href="#Research Interest">Research Interest</a>
      <a href="#Selected Works">Selected Works</a>
      <a href="#Publications">Publications</a>
      <a href="#ToolDownload">Data Collection Tools</a>
      <a href="#EULA">User License Agreement</a>
      <a href="#Fun">Fun</a>
    </div>
    <div class="main">
      <p>I am a Principal Researcher at Microsoft Research (MSR), Redmond, WA. 
        My current research interests are in deep learning and embodied multi-modal AI. 
        More specifically, my primary researches particularly focused on building generalist embodied multi-modal foundations towards AGI. 
        
      </p>
      
      <h3 id="Research Interest">Research Interest</h3>
      
        <p>
          Broadly, my work focuses on embodied AI agents with lagre foundation model generation, and how to inform machines about the world which interact with users. 
          The research topics include infinite large/small action foundation models for embodied infrastructure (pre-training, in-contextual learning, post-training with RL, IL), 
          agent acquisition and prediction in interactive AI, and symbolic auto-GPT for multi-modality with knowledge inference reasoning. 
          The recent areas of work include large action foundation models for AI agents, large vision-language models for embodied intelligence, 
          interactive AI agents, neuro-symbolic representation for knowledge inference and reasoning, 
          and self-supervised learning for multi-modality and NLP. 
          During the past year, my research at MSR has centered around the theme of AI agent 
          with pre-trained large action-vision-language foundation Models with knowledge reasoning and retrieval.
        </p>

        <p>          
          I believe, by integrating fine-grained structured information, we can achieve better yet interpretable, grounded and robust multi-modality intelligent agents for cross-modality and agnostic-reality.
          Beyond pushing state-of-the-art research, I am a strong proponent of efficient and reproducible research. We also helped to ship AI techniques to Microsoft products and to create real-world impact.
          I enjoy the process of bridging the gap between theory and practice, bringing theoretical results to practical applications. 
          I also enjoy coding, building real systems from scratch, and making them work simply, efficiently, and elegantly. 
        </p>
      
      <h3 id="Selected Works">Selected Works</h3>
        <h4 id="Task1">My research is targeted at research & practical applications involving:</h4>
          <p>
          <b>• Embodied Action Foundation Model:</b> Large embodied foundation models for flexible AI agent infrastructure with vision-language. 
          An Interactive Agent Foundation Model of large action-vision-language per-training for embodied AI in Robotics, Gaming, 
          and Embodied Healthcare. The model developed in collaboration with MS Product Robotics team, Gaming Team and Stanford Fei-fei Li’s team;
         
          <a href="https://agentfoundationmodel.github.io" class="button">Demo Link</a>
           
          </p>

         <p>
          <b>• Multi-agent Infrastructure with In-contextual Learning:</b> MindAgent: multi-agent infrastructure with GPT-4 (V) ison 
          for infinite spatial intelligence in real and virtual world, in collaboration with MS X-box team, and shipped to MS Gaming team; 
           <a href="https://mindagent.github.io" class="button">Demo Link</a>
           
          <p style="text-align: justify; font-size: 1.0em;">We present <tt>MindAgent</tt>, an infrastructure for emergent gaming interaction. 
           enables<span style="color:#002156;"> <b>multi agents collaboration</b></span> and <span style="color:#002156;">
           <b>human-agent collaboration</b></span>.
          </p>      
          </p>

        <p>
          <b>• Post-Training for Generative AI:</b> Post-Training for Generative AI:dynamic interaction model generation (GenAI) in multimodality via emergent abilities;ArK: 
          Augmented reality with emergent infrastructure Post-training using reinforcement and imitation learning for generative AI (GPT, Dall-E), 
          in collaboration with MS Mash team; the model has been shipped to MS office teams;
                 
        <a href="https://arxiv.org/pdf/2305.00970" class="button">Link</a>
               
        </p>
      
        <p>
          <b>• Applications for Human-Machine Interaction:</b> Embodied AI with RL/IL. Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning 
          for Vision-Language Navigation for simulation agent robotics, in collaboration with MS Cognition Services team;
                  
        <a href="https://arxiv.org/pdf/1811.10092" class="button">Link</a>
                
        </p>  

         <p>
          <b>• Vision-Language Pre-training:</b> Training Vision-Language Transformers from Captions, 
           Vision-language pre-training model generation in collaboration with Turing team and Cognition Services;
                   
         <a href="https://arxiv.org/abs/2205.09256" class="button">Link</a>
                 
         </p> 

        <p>
          <b>• Knowledge Inference and Representation in Multi-modality:</b> Knowledge-Inference Reasoning Transformer: 
          <div>          
          i) KAT: A Knowledge Augmented Transformer for Vision-and-Language, <a href="https://arxiv.org/abs/2112.08614" class="button">Link</a>
          </div> 
          <div>          
          ii) Retrieve What You Need: knowledge-LLM agent with GPT, <a href="https://aclanthology.org/2024.tacl-1.14/" class="button">Link</a>
          </div>
          <div>          
          and 
          iii) Logical Transformer, which collaborated with MS Turing Team and Bing search. 
          <a href="https://aclanthology.org/2023.findings-acl.111.pdf" class="button">Link</a>
          </div>        
        </p> 
    
                  
      
        <h3 id="Publications">Selected Publications</h3>
        <p>
          <b>■ An Interactive Agent Foundation Model</b> for embodied interaction in Robot, Gaming, and Healthcare.
        <div>
          Zane Durante, Bidipta Sarkar, Ran Gong, Rohan Taori, Yusuke Noda, Paul Tang, 
          Ehsan Adeli, Shrinidhi Kowshika Lakshmikanth, Kevin Schulman, Arnold Milstein, Demetri Terzopoulos, Ade Famoti, 
          Noboru Kuno, Ashley Llorens, Hoi Vo, Katsu Ikeuchi, Li Fei-Fei, Jianfeng Gao, Naoki Wake, Qiuyuan Huang
        </div>          
          arXiv:2402.05929, May 2024.
          <a href="https://arxiv.org/abs/2402.05929">[paper]</a>
          <a href="https://agentfoundationmodel.github.io">[webpage]</a>           
         </p>

        <p>
          <b>■ Agent AI Towards a Holistic Intelligence</b>
        <div>
          Qiuyuan Huang, Naoki Wake, Bidipta Sarkar, Zane Durante, Ran Gong, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, 
          Noboru Kuno, Ade Famoti, Ashley Llorens, John Langford, Hoi Vo, Li Fei-Fei, Katsu Ikeuchi, Jianfeng Gao.        
        </div>          
          arXiv:2403.00833, May 2024.          
          <a href="https://arxiv.org/abs/2403.00833">[paper]</a>        
    </p>

        <p> 
          <b>■ MindAgent: Emergent Gaming Interaction</b>
        <div>Ran Gong*, Qiuyuan Huang*, Xiaojian Ma*, Hoi Vo, Zane Durante, Yusuke Noda, Zilong Zheng, 
          Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao                 
        </div>          
        Proceedings of NAACL 2024, Jun. 2024.          
          <a href="https://arxiv.org/abs/2309.09971">[paper]</a>
          <a href="https://mindagent.github.io">[webpage]</a>         
    </p>  

    <p> 
          <b>■ Agent AI: Surveying the Horizons of Multimodal Interaction</b>
        <div>Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, 
          Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, Katsushi Ikeuchi, Hoi Vo, Li Fei-Fei, Jianfeng Gao
        </div>          
        arXiv:2401.03568, Jan 2024.
          <a href="https://arxiv.org/abs/2401.03568">[paper]</a>         
    </p> 

    <p> 
          <b>■ Localized Symbolic Knowledge Distillation: Infinite Knowledge Distillation in Multi-modality (Knowledge-auto-GPT)”</b>
        <div>Jae Sung Park, Jack Hessel, Khyathi Raghavi Chandu, Paul Pu Liang, Ximing Lu, Peter West, Youngjae Yu, Qiuyuan Huang, 
          Jianfeng Gao, Ali Farhadi, Yejin Choi
        </div>          
        Proceedings of Proceedings of NeurIPS 2023. Dec.2023
          <a href="https://arxiv.org/abs/2312.04837">[paper]</a>        
    </p>  

    <p> 
          <b>■ KAT: A Knowledge Augmented Transformer for Vision-and-Language</b>  <span style="color:#891000;"><b> SoTA of OK-VQA Leaderboard (2021).</b>
        <div>Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, Jianfeng Gao
        </div>          
        Proceedings of NAACL 2022, Jun. 2022.  
          <a href="https://arxiv.org/abs/2112.08614">[paper]</a>   
      </p>  

    
       <p> 
          <b>■ Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation</b> 
            <span style="color:#891000;"><b> Best Student Paper Award.</b>
        <div>Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang
        </div> 
        Proceedings of CVPR 2019,Jun. 2019. <a href="https://arxiv.org/abs/1811.10092">[paper]</a>    
       </p>  

      <p> 
          <b>■ Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations</b>
        <div> Kezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul Smolensky, Kenneth D. Forbus, Jianfeng Gao
        </div>          
         Proceedings of ICML 2020. Feb.2020          
      <a href="https://arxiv.org/abs/1910.02339">[paper]</a>   
      </p>  

      <p> 
          <b>■ TP-N2F: Tensor Product Representation for Natural To Formal Language Generation”</b> <span style="color:#891000;"><b>Best Paper Award.</b>
        <div>Kezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul Smolensky, Kenneth D. Forbus, Jianfeng Gao. </div>          
          Proceedings of NeurIPS workshop 2019. Dec.2019.  
          <a href="https://arxiv.org/abs/1910.02339v2">[paper]</a>   
      </p>  



      
      
      <h3 id="ToolDownload">Data Collection Tools Download</h3>
        <p>
          <b>Data Collection: </b> For our released <a href="https://nicedataset.github.io">“NICE” dataset and banchmark</a>, <a href="https://github.com/mindagent/mindagent">“Cuisine world” dataset, benchmark, and infrastructure</a> etc. You can find the codes for data collection below: 
        </p>
        <p><a href="https://github.com/NICEdataset/DataCollectionTool" class="button">Data Collection Tools</a></p>
       
      
      <h3 id="EULA">User License Agreement (EULA)</h3>
        <p>
          You should complete an end user license agreement (EULA) before accessing the dataset/benchmark/infractrcuture. 
          The EULA will be posted soon once the data releasing process is finished. 
        </p>
        <a href="https://github.com/nicedataset/NICEdataset.github.io/blob/master/End%20User%20License%20Agreement.pdf" class="button">Download EULA</a>
      
      
      <h3 id="Fun">Fun</h3>
        <p>
        <p>Ballet, Tennis, Hiking</p>
        </p>
        
        
    </div>
  </body>

</html>
