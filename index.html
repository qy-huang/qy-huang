<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Qiuyuan Huang</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="topnav">
      <a href="index.html">Qiuyuan Huang</a>
      <div class="topnav-right">
        <img src="Microsoft-Research-logo.jpg"  width="240" height="135" alt="Microsoft Research">
      </div>
    </div>
    <div class="header">
      <p></p>
    </div>
    <div class="sidenav">
      <a href="#Research Interest">Research Interest</a>
      <a href="#Work Experimence & Education">Work Experimence & Education</a>
      <a href="#Publications">Publications</a>
      <a href="#ToolDownload">Data Collection Tools</a>
      <a href="#EULA">User License Agreement</a>
      <a href="#DataDownload">Dataset Download</a>
      <a href="#Models">Model Implementation</a>
      <a href="#External">External Resources</a>
    </div>
    <div class="main">
      <h4>I am a Principal Researcher at Microsoft Research (MSR), Redmond, WA. 
        My current research interests are in deep learning and embodied multi-modal AI. 
        More specifically, my primary researches particularly focused on building generalist embodied multi-modal foundations towards AGI. 
        The recent areas of work include large action foundation models for AI agents, large vision-language models for embodied intelligence, 
        interactive AI agents, neuro-symbolic representation for knowledge inference and reasoning, 
        and self-supervised learning for multi-modality and NLP.
      </h4>
      
      <h3 id="Research Interest">Research Interest</h3>
      
        <p>
          Broadly, my work focuses on embodied AI Agents and how to inform machines about the world which interact with users. 
          The research topics include infinite large/small action foundation models for embodied infrastructure (in-contextual learning, RL, IL), 
          agent acquisition and prediction in interactive AI, and symbolic auto-GPT for multi-modality. 
          During the past year, my research at MSR has centered around the theme of AI Agent with Pre-trained Large Action-Vision-language Foundation Model with knowledge reasoning and retrieval.
          My research is targeted at research & practical applications involving:
        </p>
      
        <p>
          <b>•Embodied Action Foundation Model:</b> Large embodied foundation models for flexible AI agent infrastructure with vision-language. 
          An Interactive Agent Foundation Model of large action-vision-language per-training for embodied AI in Robotics, Gaming, 
          and Embodied Healthcare. The model developed in collaboration with MS Product Robotics team, Gaming Team and Stanford Fei-fei Li’s team;
        </p>
      
        <p>
          <b>• Multi-agent Infrastructure with In-contextual Learning:</b> MindAgent: multi-agent infrastructure with GPT-4 (V) ison 
          for infinite spatial intelligence in real and virtual world, in collaboration with MS X-box team, and shipped to MS Gaming team;        
        </p>

        <p>
          <b>• Post-Training for Generative AI:</b> Post-Training for Generative AI:dynamic interaction model generation (GenAI) in multimodality via emergent abilities;ArK: 
          Augmented reality with emergent infrastructure Post-training using reinforcement and imitation learning for generative AI (GPT, Dall-E), 
          in collaboration with MS Mash team; the model has been shipped to MS office teams;
        </p>
      
        <p>
          <b>• Applications for human-machine interaction</b> Embodied AI with RL/IL. Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning 
          for Vision-Language Navigation for simulation agent robotics, in collaboration with MS Cognition Services team;        
        </p>  

         <p>
          <b>• Vision-Language Pre-training:</b> Training Vision-Language Transformers from Captions, 
           Vision-language pre-training model generation in collaboration with Turing team and Cognition Services;
        </p> 

        <p>
          <b>• Knowledge inference and representation in multi-modality:</b> Knowledge-Inference Reasoning Transformer: 
          i) KAT: A Knowledge Augmented Transformer for Vision-and-Language, 
          ii) Retrieve What You Need: knowledge-LLM agent with GPT, and 
          iii) Logical Transformer, which collaborated with MS Turing Team and Bing search.        
        </p> 

        <p>          
          I believe, by integrating fine-grained structured information, we can achieve better yet interpretable, grounded and robust multi-modality intelligent agents for cross-modality and agnostic-reality.
          Beyond pushing state-of-the-art research, I am a strong proponent of efficient and reproducible research. We also helped to ship AI techniques to Microsoft products and to create real-world impact.
          I enjoy the process of bridging the gap between theory and practice, bringing theoretical results to practical applications. 
          I also enjoy coding, building real systems from scratch, and making them work simply, efficiently, and elegantly. 
        </p>
      
      <h3 id="Work Experimence & Education">Work Experimence & Education</h3>
        <h4 id="Task1">Task Setting I</h4>
          <p>
           <b>Task Formulation: </b> We define NICE-Setting I as generating a set of comments for an image. Formally, 
           the generation task as follows: given an image I, and N comments C<sub>1</sub>, ..., C<sub>n</sub>.
           Systems aim to generate the comment C<sub>k</sub>, where k is from 1 to N using the current state
           information S(C<sub>k</sub> | I, C<sub>1</sub>,...,C<sub>k-1</sub>). 
          </p>

        <h4 id="Task2">Task Setting II</h4>
          <p>
           <b>Task Formulation: </b> We define the NICE-Setting II as controllable generation in response to an image, similar to a 
           dialog response in a social conversation setting in order to maximize user engagement and eventually form 
           long-term, emotional connections with users. We formalize the generation task as follows: each sample of 
           this dataset has an image I, a comment topic H of the whole dialogue, and N comments 
           C<sub>1</sub>, ..., C<sub>n</sub> with corresponding affect distribution features A<sub>1</sub>, ..., A<sub>n</sub>. 
           Systems aim to generate the comment C<sub>k</sub> using the current state information 
           S(I, H, C<sub>1</sub>,...,C<sub>k-1</sub>, A<sub>k</sub>), which contains the input image I, comment
           topic H, the comments history (C<sub>1</sub>, ..., C<sub>k-1</sub>), and is conditioned on the affect feature A<sub>k</sub>.
          </p>
        
        <h4 id="Evaluation">Evaluation</h4>
          <p>
            Generated comments can be evaluated automatically on three aspects: token matching, semantic similarity, and diversity. As image comments are 
            different from traditional text generation task, generating same text as ground truth is not our goal. We propose three aspects to 
            show the quality of generated comments. We also provide a evaluation set with over 28000 human annotated labels. We suggest users to refer them 
            for human evaluation.
          </p>
          <p>
            <b>Token Matching: </b> Token matching aspect aims to evaluate whether the generated comments are similar as ground-truth comments on token level.
            Token matching quality is evaluated by Bleu score, ROUGE score, and CIDEr score.
          </p>
          <p>
            <b>Semantic Similarity: </b> Semantic Similarity aims to evalute whether the generated comments have similar sematnic meanings or emotions. Semantic 
            Similarity is evaluated by SPICE and BertScore (BertP, BertR, BertF1).
          </p>
          <p>
            <b>Diversity: </b> Image comments can be diverse from different people's persepectives. Diversity is evaluted by Entropy and Distinct score.
          </p>
          <p>
            <b>Human Evaluation: </b> For some qualities (e.g., empathy or social appropriateness), there are currently no automated metrics for 
            evaluating dialogue generation models. However, these qualities are particularly important for our data in our task.
            Users can use systems to generate comments on the subset and perform human evaluation process. Users can compare the generated
            comments and the ground truth comments. 
          </p>
          
          
      
      <h3 id="Publications">Publications</h3>
        <p>
          <b>Main paper: </b> The paper is accepted in the EMNLP 2021. 
          <a href="https://www.microsoft.com/en-us/research/uploads/prod/2021/10/NICE_EMNLP2021.pdf">[pdf]</a>
        </p>
        <p>
          <b>Workshop paper: </b> The paper for this dataset is accepted in the workshop NeurIPS20 Human in the loop Dialogue systems
          <a href="https://sites.google.com/view/hlds-2020/home">NeurIPS20 Human in the loop Dialogue systems</a>:
          <a href="https://drive.google.com/file/d/1pCy1vl9qTs4pnC23gWZXIJAv2rPmGDJ0/view?usp=sharing">[pdf]</a>
          <a href="https://drive.google.com/file/d/19gHxT--43OqF2difGSUuqS_4qbCgpesn/view?usp=sharing">[appendix]</a>
        </p>
      
      <h3 id="ToolDownload">Data Collection Tools Download</h3>
        <p>
          <b>Data Collection: </b> You can find the codes for data collection below: 
        </p>
        <p><a href="https://github.com/NICEdataset/DataCollectionTool" class="button">Data Collection Tools</a></p>
        <p>
          <b>Data Cleaning: </b>  It took several researchers multiple weeks to remove sensitive content for both image
          and text filtering. We used the ``Microsoft Adult Filtering API'' to 
          remove adult, racy and gory images, we use the ``Detecting image types API'' to remove
          clip art and line drawings, we use the ``Optical Character Recognition (OCR) API'' to remove
          printed or handwritten text from the images, such as photos of license plates or containers with serial
          numbers, as well as from documents invoices, bills, financial reports, articles, and more. We also removed
          people’s names, politically sensitive language, ethnic-religious content, or other potentially offensive
          material (including inappropriate references to violence, crime and illegal substances) as the similar
          filter API for language cleaning. We will keep cleaning and maintaining it in future.
        </p>
      
      <h3 id="EULA">User License Agreement (EULA)</h3>
        <p>
          You should complete an end user license agreement (EULA) before accessing the dataset. 
          The EULA will be posted soon once the data releasing process is finished. 
        </p>
        <a href="https://github.com/nicedataset/NICEdataset.github.io/blob/master/End%20User%20License%20Agreement.pdf" class="button">Download EULA</a>
      
      <h3 id="DataDownload">NICE Dataset Download</h3>
        <p>
          We provide a small set of data samples so users can learn what the data looks like. You can download
          the sample at bottom.
        </p>
        <a href="https://drive.google.com/file/d/1w5fWu9nU0HdA_Hf7fty3eRrDrJ-aTE1-/view?usp=sharing" class="button">Download NICE samples</a>
      
        <p>
          NICE-Dataset is a vision-language dataset for image commenting. Given an image, models are required to generate human-like comments grounded on the image. NICE-Dataset has two settings. 
          You can download images and related data for each setting as the following link.
        </p>
        <a href="https://drive.google.com/drive/folders/1V6M1W8x9vCKgabE-1dCfpoHMnj0TM00z?usp=sharing" class="button">Download NICE Dataset</a>
        
        <p>
          There are three folders in the link: Images, Setting1, and Setting2. The deatails showed as following Github button.
        </p>
        <p>
          Images: We extract the image-comment pairs from website and the time period for the data is from 2011-2012. Each zip file with the year prefix (2011 or 2012) has a set of images.
        <p>
          Setting1: For this setting, a subset of data is labeled by Mechanical Turkers with 7 different categories scored from 1-7: Appropriate, Emotional, Empathetic, Engaging, Relevant, Offensive, Selected. We provide a set of data for validation.
        </p>
        <p>
          Setting2: For the two trainval_liwc_6x6__NoBadimg_Cleaned.tsv files.
        </p>
        <a href="https://github.com/ckzbullbullet/NICE" class="button">GitHub Link</a>
        
      
      
      
      
      <h3 id="Models">Model Implementation</h3>
        <p>
          You can find the MAGIC model implementation <a href="https://github.com/NICEdataset/NICE-Pretraining-Model---MAGIC">here</a>.
        </p>
      
      <h3 id="External">External Resources</h3>
        <p>
          External researches that use NICE dataset is listed below (The list will be update):
        </p>
        <p>
          <a href="https://github.com/ckzbullbullet/NICE">https://github.com/ckzbullbullet/NICE</a>
        </p>
        
    </div>
  </body>

</html>
