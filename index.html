<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Qiuyuan Huang</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="topnav">
      <a href="index.html">Qiuyuan Huang</a>
      <div class="topnav-right">
        <img src="Microsoft-Research-logo.jpg"  width="240" height="135" alt="Microsoft Research">
      </div>
    </div>
    <div class="header">
      <p></p>
    </div>
    <div class="sidenav">
      <a href="#Research Interest">Research Interest</a>
      <a href="#Work Experimence & Education">Work Experimence & Education</a>
      <a href="#Publications">Publications</a>
      <a href="#ToolDownload">Data Collection Tools</a>
      <a href="#EULA">User License Agreement</a>
      <a href="#DataDownload">Dataset Download</a>
      <a href="#Models">Model Implementation</a>
      <a href="#External">External Resources</a>
    </div>
    <div class="main">
      <h4>I am a Principal Researcher at Microsoft Research (MSR), Redmond, WA. 
        My current research interests are in deep learning and embodied multi-modal AI. 
        More specifically, my primary researches particularly focused on building generalist embodied multi-modal foundations towards AGI. 
        The recent areas of work include large action foundation models for AI agents, large vision-language models for embodied intelligence, 
        interactive AI agents, neuro-symbolic representation for knowledge inference and reasoning, 
        and self-supervised learning for multi-modality and NLP.</h4>
      
      <h3 id="Research Interest">Research Interest</h3>
      
        <p>
          Broadly, my work focuses on embodied AI Agents and how to inform machines about the world which interact with users. 
          The research topics include infinite large/small action foundation models for embodied infrastructure (in-contextual learning, RL, IL), 
          agent acquisition and prediction in interactive AI, and symbolic auto-GPT for multi-modality. 
          My research is targeted at research & practical applications involving: 
          1) large embodied foundation models for flexible AI agent infrastructure with vision-language; 
          2) knowledge inference and representation in multi-modality; 
          3) dynamic interaction model generation (GenAI) in multimodality via emergent abilities; 
          4) applications for human-machine interaction,in robotics, gaming/VR/AR/mix-reality, embodied healthcare, and general multi-modality. 
          I believe, by integrating fine-grained structured information, we can achieve better yet interpretable, grounded and robust multi-modality intelligent agents for cross-modality and agnostic-reality.
During the past year, my research at MSR has centered around the theme of AI Agent with Pre-trained Large Action-Vision-language Foundation Model with knowledge reasoning and retrieval. 
          Beyond pushing state-of-the-art research, I am a strong proponent of efficient and reproducible research. I also helped to ship AI techniques to Microsoft products and to create real-world impact.        
        </p>
      
        <p>
          <b>Data Construction: </b> We scraped 10 million image-comment pairs from licensed website. Each thread was required 
          to start with an image and contain at least one comment. We applied filters to both the images 
          and comments to remove sensitive content such as adult or pornographic content, racy and gory 
          content, non-English language, ethnic-religious content, and some sensitive content (including 
          people's name, documents invoices, bills, financial reports) or other potentially offensive or 
          contentious material (including inappropriate references to violence, crime and illegal substances).
          After filtering, the number of images of the dataset was reduced to 2,233,926 samples and the number
          of image-comment pairs was reduced to 7,304,680 samples. In last step,  we only keep no more
          than 5 corresponding comments in each thread, and no more than 6 different dialogue threads 
          for the same image. We also filter out image-comments pairs that do not have affect
          features or dialogue topics from dialogue thread. Finally, we have 6,550,526 training samples, 100,000 validation samples,
          and 70,000 testing samples. We also had human labelers annotate a large set (over 28,000) of images and
          comments. As a single image can have multiple comment threads we randomly selected one comment thread for each image per HIT.
          In total, 28,392 image and comment samples were labeled.
        </p>
      
        <p>
          <b>Human annotation subset: </b> We also ask human labelers to annotate a large set (over 28,000) of images and
          comments. This subset is provided to users for human evaluation.
          As a single image can have multiple comment threads, we randomly selected one comment thread for each image per HIT.
          In total, 28,392 image and comment samples were labeled.
          During each Human Intelligence Task (HIT), we showed a labeler an 
          image accompanied by a comment from a single thread associated with the image. 
          The labeler was asked to rate how socially appropriate, empathetic, emotional, engaging and relevant to the image
          the comments were. Each rating was performed on a scale of 1 (not at all) to 7 (extremely). They were also asked whether 
          the text featured offensive content (No/Yes). We compensated labelers at a calculated rate of $15 per hour. 
          We also provide this subset for people to download. A screenshot of the labeling task is shown in the image below.
        </p>
        <p><center><img src="MturkTask.png" width="600" height="500" alt="Screenshot of the labeling task."></center></p>

      
      <h3 id="Work Experimence & Education">Work Experimence & Education</h3>
        <h4 id="Task1">Task Setting I</h4>
          <p>
           <b>Task Formulation: </b> We define NICE-Setting I as generating a set of comments for an image. Formally, 
           the generation task as follows: given an image I, and N comments C<sub>1</sub>, ..., C<sub>n</sub>.
           Systems aim to generate the comment C<sub>k</sub>, where k is from 1 to N using the current state
           information S(C<sub>k</sub> | I, C<sub>1</sub>,...,C<sub>k-1</sub>). 
          </p>

        <h4 id="Task2">Task Setting II</h4>
          <p>
           <b>Task Formulation: </b> We define the NICE-Setting II as controllable generation in response to an image, similar to a 
           dialog response in a social conversation setting in order to maximize user engagement and eventually form 
           long-term, emotional connections with users. We formalize the generation task as follows: each sample of 
           this dataset has an image I, a comment topic H of the whole dialogue, and N comments 
           C<sub>1</sub>, ..., C<sub>n</sub> with corresponding affect distribution features A<sub>1</sub>, ..., A<sub>n</sub>. 
           Systems aim to generate the comment C<sub>k</sub> using the current state information 
           S(I, H, C<sub>1</sub>,...,C<sub>k-1</sub>, A<sub>k</sub>), which contains the input image I, comment
           topic H, the comments history (C<sub>1</sub>, ..., C<sub>k-1</sub>), and is conditioned on the affect feature A<sub>k</sub>.
          </p>
        
        <h4 id="Evaluation">Evaluation</h4>
          <p>
            Generated comments can be evaluated automatically on three aspects: token matching, semantic similarity, and diversity. As image comments are 
            different from traditional text generation task, generating same text as ground truth is not our goal. We propose three aspects to 
            show the quality of generated comments. We also provide a evaluation set with over 28000 human annotated labels. We suggest users to refer them 
            for human evaluation.
          </p>
          <p>
            <b>Token Matching: </b> Token matching aspect aims to evaluate whether the generated comments are similar as ground-truth comments on token level.
            Token matching quality is evaluated by Bleu score, ROUGE score, and CIDEr score.
          </p>
          <p>
            <b>Semantic Similarity: </b> Semantic Similarity aims to evalute whether the generated comments have similar sematnic meanings or emotions. Semantic 
            Similarity is evaluated by SPICE and BertScore (BertP, BertR, BertF1).
          </p>
          <p>
            <b>Diversity: </b> Image comments can be diverse from different people's persepectives. Diversity is evaluted by Entropy and Distinct score.
          </p>
          <p>
            <b>Human Evaluation: </b> For some qualities (e.g., empathy or social appropriateness), there are currently no automated metrics for 
            evaluating dialogue generation models. However, these qualities are particularly important for our data in our task.
            Users can use systems to generate comments on the subset and perform human evaluation process. Users can compare the generated
            comments and the ground truth comments. 
          </p>
          
          
      
      <h3 id="Publications">Publications</h3>
        <p>
          <b>Main paper: </b> The paper is accepted in the EMNLP 2021. 
          <a href="https://www.microsoft.com/en-us/research/uploads/prod/2021/10/NICE_EMNLP2021.pdf">[pdf]</a>
        </p>
        <p>
          <b>Workshop paper: </b> The paper for this dataset is accepted in the workshop NeurIPS20 Human in the loop Dialogue systems
          <a href="https://sites.google.com/view/hlds-2020/home">NeurIPS20 Human in the loop Dialogue systems</a>:
          <a href="https://drive.google.com/file/d/1pCy1vl9qTs4pnC23gWZXIJAv2rPmGDJ0/view?usp=sharing">[pdf]</a>
          <a href="https://drive.google.com/file/d/19gHxT--43OqF2difGSUuqS_4qbCgpesn/view?usp=sharing">[appendix]</a>
        </p>
      
      <h3 id="ToolDownload">Data Collection Tools Download</h3>
        <p>
          <b>Data Collection: </b> You can find the codes for data collection below: 
        </p>
        <p><a href="https://github.com/NICEdataset/DataCollectionTool" class="button">Data Collection Tools</a></p>
        <p>
          <b>Data Cleaning: </b>  It took several researchers multiple weeks to remove sensitive content for both image
          and text filtering. We used the ``Microsoft Adult Filtering API'' to 
          remove adult, racy and gory images, we use the ``Detecting image types API'' to remove
          clip art and line drawings, we use the ``Optical Character Recognition (OCR) API'' to remove
          printed or handwritten text from the images, such as photos of license plates or containers with serial
          numbers, as well as from documents invoices, bills, financial reports, articles, and more. We also removed
          people’s names, politically sensitive language, ethnic-religious content, or other potentially offensive
          material (including inappropriate references to violence, crime and illegal substances) as the similar
          filter API for language cleaning. We will keep cleaning and maintaining it in future.
        </p>
      
      <h3 id="EULA">User License Agreement (EULA)</h3>
        <p>
          You should complete an end user license agreement (EULA) before accessing the dataset. 
          The EULA will be posted soon once the data releasing process is finished. 
        </p>
        <a href="https://github.com/nicedataset/NICEdataset.github.io/blob/master/End%20User%20License%20Agreement.pdf" class="button">Download EULA</a>
      
      <h3 id="DataDownload">NICE Dataset Download</h3>
        <p>
          We provide a small set of data samples so users can learn what the data looks like. You can download
          the sample at bottom.
        </p>
        <a href="https://drive.google.com/file/d/1w5fWu9nU0HdA_Hf7fty3eRrDrJ-aTE1-/view?usp=sharing" class="button">Download NICE samples</a>
      
        <p>
          NICE-Dataset is a vision-language dataset for image commenting. Given an image, models are required to generate human-like comments grounded on the image. NICE-Dataset has two settings. 
          You can download images and related data for each setting as the following link.
        </p>
        <a href="https://drive.google.com/drive/folders/1V6M1W8x9vCKgabE-1dCfpoHMnj0TM00z?usp=sharing" class="button">Download NICE Dataset</a>
        
        <p>
          There are three folders in the link: Images, Setting1, and Setting2. The deatails showed as following Github button.
        </p>
        <p>
          Images: We extract the image-comment pairs from website and the time period for the data is from 2011-2012. Each zip file with the year prefix (2011 or 2012) has a set of images.
        <p>
          Setting1: For this setting, a subset of data is labeled by Mechanical Turkers with 7 different categories scored from 1-7: Appropriate, Emotional, Empathetic, Engaging, Relevant, Offensive, Selected. We provide a set of data for validation.
        </p>
        <p>
          Setting2: For the two trainval_liwc_6x6__NoBadimg_Cleaned.tsv files.
        </p>
        <a href="https://github.com/ckzbullbullet/NICE" class="button">GitHub Link</a>
        
      
      
      
      
      <h3 id="Models">Model Implementation</h3>
        <p>
          You can find the MAGIC model implementation <a href="https://github.com/NICEdataset/NICE-Pretraining-Model---MAGIC">here</a>.
        </p>
      
      <h3 id="External">External Resources</h3>
        <p>
          External researches that use NICE dataset is listed below (The list will be update):
        </p>
        <p>
          <a href="https://github.com/ckzbullbullet/NICE">https://github.com/ckzbullbullet/NICE</a>
        </p>
        
    </div>
  </body>

</html>
