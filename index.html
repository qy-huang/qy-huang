<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Qiuyuan Huang</title>
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="topnav">
      <div class="topnav-left">
        <img src="Q5.png"  width="115" height="115" alt="Microsoft Research">
         <a href="index.html">Qiuyuan (Enno) Huang</a>      
      </div>            
    </div>    <div class="header">
      <p></p>
    </div>
    <div class="sidenav">
      <a href="#Research Interest">Research Interest </a>
      <a href="#Selected Works">Selected Works</a>
      <a href="#Publications">Publications</a>
      <a href="#ToolDownload">Data-Driven Tools</a>
      <a href="#EULA">User License Agreement</a>
      <a href="#Fun">Fun</a>
    </div>
    <div class="main">
      <p>I am a Principal Researcher at Microsoft Research (MSR), Redmond, WA. 
        My current research interests are in deep learning in general, and embodied AI with trustworthy multi-modal complex intelligence in particular. 
        More specifically, my primary researches particularly focused on building generalist embodied AI foundation model for autonomous multimodal system
        agent, with data-driven neurocomputation innovation, in trustworthy human empathy and society value alignment of interdisciplinary research, towards AGI.
        
      </p>
      
      <h3 id="Research Interest">Research Interest </h3>
      <h5 <b> <span style="color:#9dbad1;">
        <div> PS:<b><span style="color:#9c0a1c;"> Confidential Projects </b> are NOT listed on the website, which include but not limited to: </div>
        <div>• MSR-OpenAI Confidential Project [embodied action-multimodality pretraining, finetune, post-training, in-contextual learning with RL/IL/MPC/Decision-making] ; </div>
        <div>• Trustworthy Autonomous System [sim-to-real] for Spatial and Temporal Intelligence in Human-in-the-loop [mimic human behavior] with Human Value Alignment; </div>
        <div>• Data-driven Neuro-computation HCI [cognition, bio-neuroscience, healthcare, etc.] in Dynamic Spatial Environment [humaniod robotics, bio-wearable machine (embodied cognition), aerospace navigation, etc.]. </div>
        </b> 
      </h5>
        <p>
          Broadly, my work focuses on embodied AI with lagre multi-modal foundation models generation, and how to inform machines about the world which interact with humans. 
          The research areas include infinite embodied foundation model generation for AI agents (pre-training, in-contextual learning, post-training with RL and IL), 
          and neural-symbolic interpretable representation for knowledge inference reasoning with vision-language agent acquisition and prediction. 
          The recent topics of work include:
          <p>
          <b> <span style="color:#9dbad1;"><b> ♢ Foundation Models: Generalist Embodied Foundation Model with Trustworthy Multimodal Agent Intelligence </b></b>
            <div> • Embodied Foundation Model (pertraining, finetune, post-training, in-contextual learning) <a href="https://arxiv.org/pdf/2402.05929">[paper1]，
              <a href="https://arxiv.org/abs/2403.00833">[paper2]，<a href="https://arxiv.org/abs/2401.03568">[paper3]</a>
            </a></a> for 
              <div> &nbsp;&nbsp;i) Robotics [manupulation, navigation, gesture, grasp, locomotion, teleoperation, execution];</div> 
              <div> &nbsp;&nbsp;ii) Spatial and Temporal intelligence for 2D/3D/VR/AR/Gaming/Mix-reality [simulation and real world models and agents]; </div>
              <div> &nbsp;&nbsp;iii) AVL/VLM/LLM for genelist vision-lanaguge task [generactive AI, video-audio-language, visual-captioning, VQA, 2D/3D environment generation/editing, classification tasks, etc.; </div>
              <div> &nbsp;&nbsp;iv) Embodied human-in-the-loop foudation modeling with neural-symbolic computation，healthcare, congnition in dynamic latent space. </div>
              </div>
            <div> • Trustworthy Interactive AI Agents System Generation and Multi-agent Infrastructure Optimization in Sim2Real (simulation models to real robotics/VR/gaming/3D transferring); 
              <a href="https://arxiv.org/pdf/2309.09971">[paper]</a></div>
            <div> • Generactive AI with Action-Vision-Language Model (AVL), vision-lanaguage model (VLM), and large-language-model (LLM); <a href="https://arxiv.org/abs/2305.00970">[paper]</a> <div> 
            <div> • Augmented AVL/LLM/VLM with logical and knowledge inference reasoning interpratibility for Huamn-in-the-loop Interaction. <a href="https://aclanthology.org/2023.findings-acl.111.pdf">[paper]</a> <div> 
          </p>
          <p>
          <b> <span style="color:#9dbad1;"><b>♢ Algorithms: Embodied AI with Reinforcement Learning (RL), Imitation Learning (IL) 
            , Model Predictive Control (MPC), Generative Adversarial Network (GAN), and Decision-Making </b></b>
            <div> • Embodied Autonomous Foundation Model for Generative AI with RL/IL/MPC; <a href="https://arxiv.org/abs/2305.00970">[paper]</a> 
              – the model has been shipped to MS Product team, with Open AI.</div>
            <div> • Embodied AI with RL/IL/MPC: Vision-Language Navigation Policy Learning and Adaptation for simulation agent robotics;<a href="https://ieeexplore.ieee.org/document/8986691“>[paper]</a>
              – IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2020;</div>
            <div> • Generative AI for Human-in-the-loop intelligence of LLM/VLM with RL: Hierarchically Structured Reinforcement Learning for Topically Coherent Visual Story Generation; <a href="https://arxiv.org/abs/1805.08191“>[paper]</a>
              – Proceedings of AAAI 2019. Oral;</div>                                                                                                                             
            <div> • Embodied AI for Vision-Audio-Language with RL/GAN;</div>
            <div> • Multi-Agent Embodied System with Optimization Inference in Model Predictive Control (MPC) and In-contextual Learning for collaborative, multi-modal mobility. </div> 
          </p>
          <p>
          <b> <span style="color:#9dbad1;"><b>♢ Autonomous System: Spatial Interactive Intelligence with Dynamic Control Complexes Interpretability [Sim-to-Real] </b></b>
            <div> • Embodied Dynamics Autonomous Systems, which conducts research in modeling and control of distributed parameters systems, 
              with applications to Autonomous Navigation, Robotics Systems (action perdiction, manupulation, motion planning and coordination, aerospace vehicle, state estimation and localization, etc.) ; </div>
            <div> • Infinite Dimensional Systems for Humaniod Robotics, Aerospace Navigation, and Wearable Divice with Cognition and Bio-Neurocomputation (mimic human behavior, manupulation, and emotion) 
              on Complex Intelligence. </div>  
            <div> • Tensor Product Representation of Inference Neural-Symbolic Foundation Model with Neuroscience Interpretability. </div>
          </p>

          <p>
          <b> <span style="color:#9dbad1;"><b>♢ Dataverse Alignment: Trustworthy Data-driven Interdisciplinary Innovation for Human Empathy and Society Value Alignment. </b></b>
            <div> • Knowledge Inference Reasoning Representation for Large Foundation Models in Cognition Complex with Trustworthy Dataverse
              (mimic human behavior and inference-interpretability); </div>
            <div> • Embodied AI of Data mining, Neural-symbolic computational representation, Healthcare, Behavioral science, Human cognition, Distributed system governance, in the cutting-edge frontiers research 
              for Mimicking Human Behavior with Empathy Alignment of across-disciplinary Boundaries. <div> 
          </p>
          and 
          During the past year, my research at MSR has centered around the theme of Infinite Embodied AI 
         for pre-trained large multimodal foundation models — an autonomous system that integrates action-vision-lanaguge models into interactive agnet with neurocomputation inference knowledge
        with trustworthy value alignment.
         
        </p>

        <p>          
          I believe, by integrating embodied AI with automous system on spatail and temporal distribution information, we can achieve better yet interpretable, grounded and robust multi-modality intelligent agents for cross-modality and agnostic-reality.
          Beyond pushing state-of-the-art research, I am a strong proponent of efficient and reproducible research. We also helped to ship Embodied AI techniques to Microsoft products and to create real-world impact.
          I enjoy the process of bridging the gap between theory and practice, bringing theoretical results to practical applications. 
          I also enjoy coding, building real systems from scratch, and making them work simply, efficiently, and elegantly. 
        </p>
      
      <h3 id="Selected Works">Selected Works</h3>
        <h4 id="Task1">My research is targeted at research & practical applications involving:</h4>
          <p>
          <b>• Embodied Action Foundation Model:</b> Large embodied foundation models for flexible AI agent infrastructure with vision-language. 
          "An Interactive Agent Foundation Model" of large action-vision-language per-training for embodied AI in Robotics, Gaming/mix-reality, 
          and Embodied Healthcare. The model developed in collaboration with MS Product Robotics team, Gaming Team, Turing team, and Stanford.
          <a href="https://agentfoundationmodel.github.io/pdfs/paper.pdf" class="button">Link</a>
          <a href="https://agentfoundationmodel.github.io" class="button">Demo Link</a>
           
          </p>

         <p>
          <b>• Multi-agent Infrastructure with In-contextual Learning:</b> "MindAgent: multi-agent infrastructure with GPT-4 (V) ison 
          for infinite spatial intelligence in real and virtual world", in collaboration with MS X-box team, and shipped to MS Gaming team; 
           <a href="https://mindagent.github.io" class="button">Demo Link</a>
           
          <p style="text-align: justify; font-size: 1.0em;">We present <tt>MindAgent</tt>, an infrastructure for emergent gaming interaction, 
           enables<span style="color:#f1f0f1;"> <b>multi agents collaboration</b></span> and <span style="color:#f1f0f1;">
           <b>human-agent collaboration</b></span>.
          </p>      
          </p>

        <p>
          <b>• Post-Training for Generative AI:</b> dynamic interaction model generation (GenAI) in multimodality via emergent abilities. "ArK: 
          Augmented reality with emergent infrastructure". Post-training using reinforcement and imitation learning for generative AI (GPT, Dall-E), 
          in collaboration with MS Mesh team; the model has been shipped to MS office teams;
                 
        <a href="https://arxiv.org/pdf/2305.00970" class="button">Link</a>
               
        </p>
      
        <p>
          <b>• Applications for Human-Machine Interaction:</b> Embodied Multimodal Navigation with RL & IL. "Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning 
          for Vision-Language Navigation" for simulation agent robotics, in collaboration with MS Cognition Services team;
                  
        <a href="https://arxiv.org/pdf/1811.10092" class="button">Link</a>
         <a href="https://www.microsoft.com/en-us/research/video/reinforced-cross-modal-matching-and-self-supervised-imitation-learning-for-vision-language-navigation/" class="button">Demo Link</a>                
        </p>  

         <p>
          <b>• Vision-Language Pre-training:</b> "Training Vision-Language Transformers from Captions", 
           Vision-language pre-training model generation in collaboration with Turing team and Cognition Services;
                   
         <a href="https://arxiv.org/abs/2205.09256" class="button">Link</a>
                 
         </p> 

        <p>
          <b>• Knowledge Inference and Representation in Multi-modality:</b> Knowledge-Inference Reasoning Transformer: 
          <div>          
          i) "KAT: A Knowledge Augmented Transformer for Vision-and-Language", <a href="https://arxiv.org/abs/2112.08614" class="button">Link</a>
          </div> 
          <div>          
          ii) "Retrieve What You Need: knowledge-LLM agent with GPT", <a href="https://aclanthology.org/2024.tacl-1.14/" class="button">Link</a>
          </div>
          <div>          
          iii) "Logical Transformer", which collaborated with MS Turing Team and Bing search. 
          <a href="https://aclanthology.org/2023.findings-acl.111.pdf" class="button">Link</a>
          </div>        
        </p> 
              
        
        <h3 id="Publications">Selected Publications </h3>          
        <h4 id="Task1">(please refer to <a href="https://scholar.google.com/citations?user=U7Mmyc8AAAAJ&hl=en" class="button">Google Scholar</a> for the full publications)</h4>          
           
        <p>
          <b>■ An Interactive Agent Foundation Model</b> for embodied interaction in Robot, Gaming, and Healthcare.
        <div>
          Z. Durante*, R. Gong*, R. Taori, Y. Noda, P. Tang, 
          E. Adeli, S. Kowshika Lakshmikanth, K. Schulman, A. Milstein, D. Terzopoulos, A. Famoti, 
          N. Kuno, A. Llorens, H. Vo, K. Ikeuchi, L. Fei-Fei, J. Gao, N. Wake*▶, Q. Huang*▶.    ∗Equal Contribution. ▶Project Lead.
        </div>          
          arXiv:2402.05929, May 2024.
          <a href="https://arxiv.org/pdf/2402.05929">[paper]</a>
          <a href="https://agentfoundationmodel.github.io">[webpage]</a>           
         </p>

        <p>
          <b>■ Agent AI Towards a Holistic Intelligence.</b>
        <div>
          Q. Huang, N. Wake, Z. Durante, R. Gong, R. Taori, Y. Noda, D. Terzopoulos, 
          N. Kuno, A. Famoti, A. Llorens, J. Langford, H. Vo, L. Fei-Fei, K. Ikeuchi, J. Gao.        
        </div>          
          arXiv:2403.00833, May 2024.          
          <a href="https://arxiv.org/abs/2403.00833">[paper]</a>        
    </p>

        <p> 
          <b>■ MindAgent: Emergent Gaming Interaction.</b>
        <div>R. Gong*, Q. Huang*▶, X. Ma*, H. Vo, Z. Durante, Y. Noda, Z. Zheng, 
          S. Zhu, D. Terzopoulos, L. Fei-Fei, J. Gao.  
          *Equal Contribution. ▶Project Lead.        
        </div>          
        Proceedings of NAACL 2024, Jun. 2024.          
          <a href="https://arxiv.org/pdf/2309.09971">[paper]</a>
          <a href="https://mindagent.github.io">[webpage]</a>         
    </p>  

    <p> 
          <b>■ Agent AI: Surveying the Horizons of Multimodal Interaction.</b>
        <div>Z. Durante*, Q. Huang*▶, N. Wake*, R. Gong, J. Park, B. Sarkar, 
          R. Taori, Y. Noda, D. Terzopoulos, Y. Choi, K. Ikeuchi, H. Vo, L. Fei-Fei, J. Gao.   *Equal Contribution. ▶Project Lead.</div>          
        arXiv:2401.03568, Jan 2024.
          <a href="https://arxiv.org/abs/2401.03568">[paper]</a>         
    </p> 

  <p> 
          <b>■ Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering.</b>
        <div>D. Wang, Q. Huang, M. Jackson, J. Gao</div>          
        Proceedings of Transactions of ACL (TACL) 2024. Apr. 2024
      <a href="https://aclanthology.org/2024.tacl-1.14/">[paper]</a>         
    </p>       

    <p> 
          <b>■ ArK: Augmented Reality with Knowledge Interactive Emergent Ability. (Generative AI)   </b>
        <div>Q. Huang, J. Park, A. Gupta, P. Bennett, R. Gong, S. Som, B. Peng, O. Mohammed, C. Pal, Y. Choi, J. Gao.         
        </div>          
        arXiv:2305.00970. Shipped the model in Microsoft Teams for product, collobarated with Microsoft with Mesh team. Jun. 2023
      <a href="https://arxiv.org/abs/2305.00970">[paper]</a> 
      <a href="https://augmented-reality-knowledge.github.io">[webpage]</a>
      <a href="https://www.microsoft.com/en-us/research/project/mixed-reality/">[page]</a> 
    </p>

      

    <p> 
          <b>■ Localized Symbolic Knowledge Distillation: Infinite Knowledge Distillation in Multi-modality (Knowledge-auto-GPT)</b>
        <div>J. Park, J. Hessel, K. Chandu, P. Liang, X. Lu, P. West, Y. Yu, Q. Huang, 
          J. Gao, A. Farhadi, Y. Choi
        </div>          
        Proceedings of NeurIPS 2023. Dec.2023
          <a href="https://arxiv.org/abs/2312.04837">[paper]</a>        
    </p>  

        <p> 
          <b>■ Logical Transformers: Infusing Logical Structures into Pre-Trained Language Models</b> 
        <div>B. Wang, Q. Huang, B. Deb, A. Halfaker, L. Shao, D. McDuff, A. Awadallah, D. Radev, J. Gao</div>          
          Proceedings of ACL 2023. Jul.2023
            <a href="https://aclanthology.org/2023.findings-acl.111.pdf">[paper]</a>   
      </p> 

        <p> 
          <b>■ Training Vision-Language Transformers from Captions (Vision-Lanaguage Pertraining)</b> 
        <div>L. Gui, Y. Chang, Q. Huang, S. Som, A. Hauptmann, J. Gao, Y. Bisk.</div>          
          Proceedings of Transactions on Machine Learning Research. 2023
      <a href="https://arxiv.org/abs/2205.09256">[paper]</a>   
      </p> 

      
      
    <p> 
          <b>■ KAT: A Knowledge Augmented Transformer for Vision-and-Language.</b> <span style="color:#9c0a1c;"><b> SoTA of OK-VQA Leaderboard (2021).</b>
        <div>L. Gui, B. Wang, Q. Huang, A. Hauptmann, Y. Bisk, J. Gao
        </div>          
        Proceedings of NAACL 2022, Jun. 2022.  
          <a href="https://arxiv.org/abs/2112.08614">[paper]</a>   
      </p> 


   <p> 
          <b>■ Vision-Language Navigation Policy Learning and Adaptation.</b> 
        <div>X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y. Wang, W. Wang, L. Zhang
        </div> 
        IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). Jun. 2020. <a href="https://ieeexplore.ieee.org/document/8986691">[paper]</a>    
       </p>      

    
       <p> 
          <b>■ Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation.</b> 
            <span style="color:#9c0a1c;"><b> Best Student Paper Award.</b>
        <div>X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y. Wang, W. Wang, L. Zhang
        </div> 
        Proceedings of CVPR 2019. Jun. 2019. <a href="https://arxiv.org/abs/1811.10092">[paper]</a>    
       </p>  

      
      <p> 
          <b>■ Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations.</b>
        <div> K. Chen, Q. Huang, H. Palangi, P. Smolensky, K. Forbus, J. Gao
        </div>          
         Proceedings of ICML 2020. Feb.2020          
      <a href="https://arxiv.org/abs/1910.02339">[paper]</a>   
      </p>  

      <p> 
          <b>■ TP-N2F: Tensor Product Representation for Natural To Formal Language Generation.</b> <span style="color:#9c0a1c;"><b>Best Paper Award.</b>
        <div>K. Chen, Q. Huang, H. Palangi, P. Smolensky, K. Forbus, J. Gao. </div>          
          Proceedings of NeurIPS workshop 2019. Dec.2019.  
          <a href="https://arxiv.org/abs/1910.02339v2">[paper]</a>   
      </p>  


       <p> 
          <b>■ Attentive Tensor Product Learning.</b> 
        <div>Q. Huang, L. Deng, O. Wu, C. Liu, X.He</div>          
          Proceedings of AAAI 2019. Feb. 2019
          <a href="https://arxiv.org/abs/1802.07089">[paper]</a>   
      </p> 

       <p> 
          <b>■ Turbo Learning for CaptionBot and DrawingBot.</b> 
        <div>Q. Huang, P. Zhang, D. Wu, L. Zhang.</div>          
          Proceedings of NeurIPS 2018. Dec.2018.  
          <a href="https://arxiv.org/abs/1805.08170">[paper]</a>   
      </p>  

        <p> 
          <b>■ Hierarchically Structured Reinforcement Learning for Topically Coherent Visual (video) Story Generation.</b> 
        <div> Q. Huang*, Z. Gan*, A. Celikyilmaz, L. Li, D. Wu, X. He. *Equal contribution. </div>          
          Proceedings of AAAI 2019. Feb.2019          
       <a href="https://arxiv.org/abs/1805.08191">[paper]</a>   
      </p> 

       <p> 
          <b>■ Tensor Product Generation Networks for Deep NLP Modeling.</b> 
        <div>Q. Huang, P. Smolensky, X. He, L. Deng, D. Wu, Long paper.</div>          
          Proceedings of NAACL 2018, ACL–Association for Computational Linguistics, New Orleans, Louisiana, US. Jun.1- Jun.6. 2018.
     <a href="https://aclanthology.org/N18-1114/">[paper]</a>   
      </p> 

         <p> 
          <b>■ AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks.</b> 
        <div>T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, X. He.</div>          
          Proceedings of CVPR 2018. Jun.2018          
      <a href="https://arxiv.org/abs/1711.10485">[paper]</a>   
      </p> 

          <p> 
          <b>■ Martian--message broadcast via LED lights to heterogeneous smartphones: poster.</b> <span style="color:#9c0a1c;"><b>Best Poster Paper Award.</b>        
            <div> H. Du, J. Han, Q. Huang, X. Jian, C. Bo, Y. Wang, X.-Y. Li.</div>          
          Proceedings of the 22nd ACM MobiCom, 417-418. Feb. 2016
         <a href="https://dl.acm.org/doi/abs/10.1145/2973750.2985257">[paper]</a>   
          </p> 

      


      
      
      
      <h3 id="ToolDownload">Data Collection and Analyzation Tools Download</h3>
        <p>
        I am also working on the trustworthy embodied data-driven alignment for multimodal data collection, safety analysis, and related Benchmark generation which collaborated with MS Turing team, Robotics team, X-box
        team, University of Washington, and Stanford University, Sanctuary AI, and Open AI.
        </p>
        <p>
          <b>Data Collection and Analyzation Collection: </b> For our released <br> 
          <a href="https://nicedataset.github.io">“NICE” dataset and banchmark: embodied data-driven multimodality for mimic human action, emmotion, with human empathy and sccity value alignment.</a> <br /> 
          <a href="https://github.com/mindagent/mindagent">“Cuisine world” dataset, benchmark, and infrastructure: embodied multi-agent data-driven for human-machine-interaction in Spatial and Temporal Intelligence</a> etc. <br /> 
          You can find the codes for data collection below: 
        </p>
        <p><a href="https://github.com/NICEdataset/DataCollectionTool" class="button">Data Collection Tools</a></p>
       
      
      <h3 id="EULA">User License Agreement (EULA) for Using Our Dataset</h3>
        <p>
          You should complete an end user license agreement (EULA) before accessing the dataset/benchmark/infractrcuture. 
          The EULA will be posted soon once the data releasing process is finished. 
        </p>
        <a href="https://github.com/nicedataset/NICEdataset.github.io/blob/master/End%20User%20License%20Agreement.pdf" class="button">Download EULA</a>
      
      
      <h3 id="Fun">Fun</h3>
        <p>
        <p>Ballet, Violin, Tennis, Hiking</p>
        </p>
        
        
    </div>
  </body>

</html>
